{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TimeSeries.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/1XNxV7fKIRcu5uq8XBFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnnybigoode/practical-time-series/blob/main/TimeSeries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPGm9yP9uovQ"
      },
      "source": [
        "# Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mlXQ7mUuvQv"
      },
      "source": [
        "Starts with examples of things that form time series, like weather, stock markets and heartbeats.\n",
        "\n",
        "Reminds us of the future of IoT and the amount of sensors and gadgets available for high-quality time series data\n",
        "\n",
        "Expects a introductory statistics background\n",
        "\n",
        "There are at least two youtube 3h long scipy talks with the author on the subject\n",
        "\n",
        "https://www.youtube.com/watch?v=JNfxr4BQrLk\n",
        "\n",
        "https://www.youtube.com/watch?v=v5ijNXvlC5A&t=1s\n",
        "\n",
        "book errata\n",
        "https://www.oreilly.com/catalog/errata.csp?isbn=0636920187714\n",
        "\n",
        "github repo\n",
        "https://github.com/PracticalTimeSeriesAnalysis/BookRepo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCmMTWgdvaJr"
      },
      "source": [
        "# Chapter 1 Time Series: Overview and Quick History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD7nLu2EvkCw"
      },
      "source": [
        "Time series: Data in choronological order\n",
        "\n",
        "Time series analysis: Diagnose the past and predict the future\n",
        "\n",
        "Medical field examples:\n",
        "\n",
        "electrocardiograms - records electrig signals passing thru the heart\n",
        "electroencephalogram - measures eletrical impulses in the brain\n",
        "\n",
        "examples of time series in weather:\n",
        "\n",
        "barometer - taking meassurements of the state of the atmosphere at regular invervals\n",
        "\n",
        "examples of time series in bussiness:\n",
        "\n",
        "forecasting demand, estimaging future raw materials prices, hedging on manufacturing costs\n",
        "\n",
        "examples of modern machine learning related to time-series:\n",
        "\n",
        "computer security, anomaly detection as a method for getting hackers and invader dynamic time warping, a way of checking similarity of time series\n",
        "recursive neural networks show usefulness for extracting patterns in corrupted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0agz-oYwRb-"
      },
      "source": [
        "# Chapter 2 All About The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBAlDhguwbs2"
      },
      "source": [
        "problems related to preprocessing time serries data\n",
        "\n",
        "-finding time series online\n",
        "Kaggle!\n",
        "UCI Machine Learning Repository\n",
        "UEA and UCR Time Series Classification Repository\n",
        "NOAA National Centers for Enviromental Information\n",
        "Bureau of Labor Statiscs\n",
        "Centers for Disease Control and Prevention\n",
        "Federal Reserve Bank of St Louis\n",
        "CompEngine\n",
        "Mcomop and M4comp2018 R Packages\n",
        "CRAN Repository\n",
        "\n",
        "*time series can be related to anything related that is sorted in time, such as the spectrum of a wine, with the wavelenghts evenly spaced on the x-axis\n",
        "\n",
        "*does this mean that using Fourier we can transform things in the frequency domain, to time domain and its somehow related?\n",
        "\n",
        "*data usually must be assembled from a collection of data\n",
        "\n",
        "Lookahead: time series information that was unknown when designing, traning or evalutating a model. This can and will screw things up. There's no test for it, something one must be vigilant and thoughtful.\n",
        "\n",
        "When counting information, remember to put back the data you removed, aquelas coisas relacionadas a calular incluindo os extremos\n",
        "\n",
        "RECAPING SOME TECHNIQUES\n",
        "from worked example pg 27-35\n",
        "\n",
        "1. recalibrate data resolution. data might come daily, but we only need weekly.\n",
        "\n",
        "2. avoid lookahead by NOT using data for timestamps that produces data availability???\n",
        "\n",
        "3. record all relevant time periods even if nothing happened. a zero count is as important as any other\n",
        "\n",
        "4. avoid lookahead by NOT using data for timestamps that produce information we shiouldn't know yet about\n",
        "\n",
        "Timestamps are specially tricky to work with due to their complexity. It is important to understand what a ts means. Is it when the event happened? Or was it when it was recorded into a table? Maybe it was when the user logged into the system? Or when the data migrated tables. If you can, reading the code that generated the data might help, even better would be talking to those who wrote the code, and in a perfect world, documentation that correctly reflects code. \n",
        "\n",
        "In most cases, we'll have to do some imperical research.\n",
        "\n",
        "Analyse a single user's tss and then expand to other users. Always consider that the ts might be the user's localtime, meaning, you can notice usage behavior to be directly correlated to choronology.\n",
        "\n",
        "You can also subtract the previous entry ts from the current and look for zeroes. Several entries on the same ts indicate a server behavior.\n",
        "\n",
        "Psychological Time Discounting / Psychological distance. People are more optimistic when estimating things in the distant future\n",
        "\n",
        "Cleaning data basics:\n",
        "\n",
        "-missing data\n",
        "\timputation, fill in missing data based on the whole database\n",
        "\titerpolation, use near points to predict missing values \n",
        "\tdeletion of affected time periods, ignore time periods we don't have data\n",
        "\n",
        "R data.table has a rolling join that seems to be amazing the joining on timestamps and a zoo package that has a bunch of timeseries related functions\n",
        "\n",
        "foward fill makes more sense even if fancier methods are possible, think about what the missing data could be\n",
        "\n",
        "take notice when using moving averages and using mean deviations when inerpolating because lookaheads\n",
        "\n",
        "-upsampling and downsampling\n",
        "\tselect output every nth element\n",
        "\tto focus on seasons, pick a single month every year, or a single day every week\n",
        "\tin irregular time series, rolling join\n",
        "\t\n",
        "-smoothing\n",
        "\twhy tho?\n",
        "\tdata preparation\n",
        "\tfeature generation\n",
        "\tprediction\n",
        "\tvisualization\n",
        "\t\n",
        "it's important to check outcomes affected by smoothing\n",
        "\n",
        "using exponential smoothing (and averages on this matter) gives more weight to recent data vs old data\n",
        ">check more resources on page 69\n",
        "\n",
        "pandas function ewma for exponential smoothing\n",
        "\n",
        "smoothing is so comon that you should use it as a easy null model when testing against fancier methods\n",
        "\n",
        "starting smoothing calculations may be problematic, if you think about two points, a exponential operation would consider the first point to be the sum of knowledge of what came before\n",
        "\n",
        ">Seasonal Data\n",
        "\n",
        "to see seasonal data, a different plot might help.\n",
        "\n",
        "pg62 shows a single line of R to decompose components: seasonal, trend, remainder\n",
        "\n",
        "documentation available at\n",
        "\n",
        "https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/stl\n",
        "\n",
        "TIMEZONES: everyone hates them\n",
        "pickling timezone objects is problematic \n",
        "why? bloggers mentioned it\n",
        "\n",
        "PREVENTING LOOKAHEAD\n",
        "\n",
        "-when smoothing data, experiment\n",
        "-build everything on small data and introduce random spot checks\n",
        "-for each kind of data, check what the timestamps relate to\n",
        "-use rolling testing or cross validation, RANDOMIZING TRAINING VERSUS TESTING DATA DOES NOT WORK WITH TIME SERIES\n",
        "-intentionally introduce lookahead, if you know a model's accuracy with lookahead, it's easier to set a maximum for the model ur building\n",
        "-add features slowly, any jump in acc should be explaned by lookahead\n",
        "\n",
        ">Review of functional data\n",
        "analysis / https://perma.cc/3DNT-J9EZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33DocB8ewuY9"
      },
      "source": [
        ""
      ]
    }
  ]
}